{"cells":[{"cell_type":"markdown","metadata":{"id":"iGwxHuwMNu3r"},"source":["#Introduction\n","This is a Python code for fitting Mossbauer spectra obtained from source spectrometers with constant acceleration motions. (See: https://en.m.wikipedia.org/wiki/Mossbauer_spectroscopy).\n","\n","#Description\n","In the first cell, the necessary Numpy, Scipy, Pandas, Pathlib, and Matplotlib libraries are imported. Additionally, Lmfit is installed, which will be used to perform the linear regression. While this notebook is not required to be in the same Drive directory as the data file, it is recommended, as each fitting model requires a specific notebook. The variable img is the path of the directory and must be appropriate for the location of the data file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tS5n1PxK7rL"},"outputs":[],"source":["\"\"\"\n","Folding by FFT and smoothing with Savizky-Golay (this last, just for poor statistics spectra). Fitting by lmfit library.\n","Likewise, it is included an option to identify phases by K-Nearest Neighbors (KNN) from hyperfine parameters results comparison with a local database\n","The next lines prepare Drive connection and import necessary libraries\n","\"\"\"\n","!pip install lmfit\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount= True)\n","img = '/content/drive/MyDrive/MyDirectory/MyFile'\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from lmfit import Parameters, minimize, fit_report, Model\n","from scipy.optimize import curve_fit\n","from scipy.constants import *\n","from scipy.integrate import trapezoid as trapz\n","from scipy.signal import savgol_filter\n","from scipy.stats import norm\n","from pathlib import Path\n","\n","path= Path(img); file= path.stem; title= path.parent.name; full= path.parents[0]; print(file, title, full)"]},{"cell_type":"markdown","metadata":{"id":"g06Eoc7wNzSj"},"source":["Folding is done using the *Numpy* 'fft' library (Discrete Fourier Transform), which allows you to determine the local minimum or maximum in the central region of the Fourier spectrum, which corresponds to the folding channel, as defined by the Nyquist-Shannon theorem (https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter24.02-Discrete-Fourier-Transform.html).\n","\n","To determine the folding channel, select *idmax* or *idmin* as appropriate and adjust the neighborhood around the folding channel.\n","\n","There is an optional line, commented with the # symbol, that is used to smooth spectra with a lot of statistical noise using the Savitzky-Golay algorithm. It is recommended only for calibrations since these are usually measured for shorter periods of time. In this section, the $calib.txt$ file is generated, which will store the calibration date information, the $V_{max}$ that defines the mm/s per channel, and the number of spectrometer channels. In addition, the $.csv$ file will have two columns: $V(mm/s)$ and the folded and normalized spectrum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7iOaAYD2-fr"},"outputs":[],"source":["\"\"\"\n","Loud and folding raw datafile subsection\n","\"\"\"\n","y= np.loadtxt(img, skiprows=0); #y=y[:,1] #add delivery=\" \" or delimiter=\" \" in loadtxt if it is two columns. Otherwise comment y=y[:,1]\n","N=len(y); N2=int(N/2); N4=int(N/4)\n","\n","fecha = str(input('Enter the calibration date (YYYYMMDD): '),)\n","vel = float(input('Enter the V range (en mm/s): ' ), )\n","np.savetxt(f\"{full}/{file}-calib.txt\", (fecha,vel, N), fmt='%s')\n","\n","#y= savgol_filter(y, 5, 2) #Only if it is a spectrum with low statistics, comment otherwise\n","plt.plot(y)\n","plt.show()\n","\n","\"\"\"\n","Folding by FFT (based on Nyquist-Shannon sampling theorem, https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter24.02-Discrete-Fourier-Transform.html)\n","\"\"\"\n","f= abs(np.fft.fft(y)); Nf=pd.Series(f[N2-1:N2+1]).idxmax()+(N2-1)\n","plt.plot(f[1:N])\n","plt.show()\n","\n","for i in range(0, Nf-1):\n","    y[[i]]=y[[i]]+y[[N-1-i]]\n","\n","y=y[0:N2-1]\n","\n","\"\"\"\n","Normalisation\n","\"\"\"\n","y2= np.concatenate([y[2:30],y[N2-30:N2-2]])\n","\n","ymax=np.mean(y2)\n","ymax=int(ymax)\n","\n","for i in range(0, N2-1):\n","    y[[i]]=y[[i]]/ymax\n","\n","x=np.arange(1,N2)\n","\n","\"\"\"\n","From channels to mm/s\n","\"\"\"\n","v=np.loadtxt(f\"{full}/{file}-calib.txt\"); vmax=v[[1]]\n","\n","x=np.linspace(start = 0, stop = N2-1, num= N2)\n","\n","for i in range(0, N2-1):\n","    x[[i]]=(i-N4)*vmax/N4\n","\n","x= x[0:N2-1]\n","\n","print('Background:', ymax); print('Folding Channel:', Nf)\n","\n","plt.plot(x,y, marker=\"o\")\n","plt.show()\n","\n","np.savetxt(f\"{full}/{file}.csv\", list(zip(x,y)), delimiter=\",\", fmt='%1.6f')"]},{"cell_type":"markdown","metadata":{"id":"BkWfnkDsN_4Y"},"source":["The following cell fits the already folded and normalized spectrum in the previous cell and reports the obtained parameters. The defined fitting functions are singlets (*singlet, S*), doublets (*doublet, Q*), and sextets (*sextet, X*) for crystalline environments of the probe nucleus and distributions for disordered environments. The latter are histograms of sextets (*dist*). These functions are subtracted from the background in Transmission and summed in CEMS measurements.\n","\n","Each model requires a number of hyperfine interactions added in *y_fit*, and it is recommended to add the cardinally ordered parameters in each section.\n","\n","The determination of the areas of each subspectrum is performed using the *trapz* function in *Scipy*. \\\n","There are two output files: *plot* (multi-column with $V(mm/s)$, normalized data, subspectra, and the model total) and *report* (which contains the final hyperfine parameters of the fit for each proposed interaction). The relative percentage of each subspectra $z_{i}$ is included in the *report* output file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6YWn_7DolT2"},"outputs":[],"source":["\"\"\"\n","Reading folded datafile\n","\"\"\"\n","col_list=[0, 1]\n","\n","x, y= np.loadtxt(f\"{full}/{file}.csv\", delimiter=\",\", usecols=col_list, unpack=True)\n","\n","\"\"\"\n","Functions definition for fitting, based in lorentzian shapes\n","\"\"\"\n","# Mössbauer constants for doublet (transition level factors)\n","positions_d = np.array([-1 , 1]) / 2  # Relatives to B_hf\n","intensities_d = np.array([1, 1])  # Relative intensity for each line\n","\n","\n","# Mössbauer constants for sextets (transition level factors)\n","positions_x = np.array([-1., -3/5., -1/5., 1/5, 3/5, 1.])   # Relatives to B_hf\n","intensities_x = np.array([3, 2, 1., 1., 2, 3])  # Relative intensity for each line\n","def lorentzian(x, amplitude, center, width):\n","    \"\"\"Singlet Lorentzian Function\"\"\"\n","    return amplitude * (2*width / ((np.pi) * ((x - center)**2 + width**2)))\n","\n","def doublet_lorentzian(x, delta, quad, gamma, scale):\n","    \"\"\"\n","    Doublet model for Mössbauer spectrum of quadrupolar splitting.\n","\n","    Parameters:\n","    - delta: Isomer Shift (IS)\n","    - quad: Quadrupolar Splitting (QS)\n","    - gamma: Lorentzian function linewidth\n","    - scale: intensity scale factor\n","    \"\"\"\n","    y = np.zeros_like(x)\n","    for i, (pos, inten) in enumerate(zip(positions_d, intensities_d)):\n","        center = delta + pos * quad\n","        y += lorentzian(x, scale * inten, center, gamma)\n","    return y\n","\n","def sextet_lorentzian(x, delta, B_hf, gamma, scale):\n","    \"\"\"\n","    Sextet model for Mössbauer spectrum of Zeeman splitting.\n","\n","    Parámetros:\n","    - delta: Isomer shift (IS)\n","    - B_hf: hyperfine magnetic field\n","    - gamma: Lorentzian function linewidth\n","    - scale: intensity scale factor\n","    \"\"\"\n","    y = np.zeros_like(x)\n","    for i, (pos, inten) in enumerate(zip(positions_x, intensities_x)):\n","        center = delta + pos * B_hf\n","        y += lorentzian(x, scale * inten, center, gamma)\n","    return y\n","\n","# Lmfit for model definition and fitting\n","\n","# 1. Ask the user for the number of components\n","n_lorentz = int(input(\"How many simple Lorentzian functions do you need?\"))\n","n_doublet = int(input(\"How many Lorentzian doublets do you need?\"))\n","n_sextet = int(input(\"How many Lorentzian sextets do you need?\"))\n","\n","# 2. Create a dynamically combined model\n","combined_model = Model(lambda x: np.zeros_like(x))\n","params = Parameters()\n","\n","# 3. Function to obtain initial values ​​from the user\n","def get_initial_values(comp_type, comp_num):\n","  \"\"\"\n","  Prompts the user for initial values ​​for each parameter\n","  \"\"\"\n","    print(f\"\\n--- Configurando {comp_type} {comp_num} ---\")\n","\n","    if comp_type == 'Lorentziana':\n","        amplitude = float(input(f\"Initial amplitude for Lorentzian {comp_num}: \") or \"0.1\")\n","        center = float(input(f\"Initial center (mm/s) for Lorentziana{comp_num}: \") or \"0.0\")\n","        width = float(input(f\"Initial width (mm/s) for Lorentziana {comp_num}: \") or \"0.15\")\n","        return {'amplitude': amplitude, 'center': center, 'width': width}\n","\n","    elif comp_type == 'Doblete':\n","        delta = float(input(f\"Initial Delta (mm/s) for Doublet {comp_num}: \") or \"0.3\")\n","        quad = float(input(f\"Initial Quad (mm/s) for Doublet {comp_num}: \") or \"1.0\")\n","        gamma = float(input(f\"Initial Gamma (mm/s) for Doublet{comp_num}: \") or \"0.15\")\n","        scale = float(input(f\"Initial scale for Doublet {comp_num}: \") or \"0.2\")\n","        return {'delta': delta, 'quad': quad, 'gamma': gamma, 'scale': scale}\n","\n","    elif comp_type == 'Sexteto':\n","        delta = float(input(f\"Initial Delta (mm/s) for Sextet {comp_num}: \") or \"0.5\")\n","        B_hf = float(input(f\"Initial B_hf for Sextet (mm/s) {comp_num}: \") or \"2.5\")\n","        gamma = float(input(f\"Initial Gamma (mm/s) for Sextet {comp_num}: \") or \"0.25\")\n","        scale = float(input(f\"Initial scale for Sextet {comp_num}: \") or \"0.2\")\n","        return {'delta': delta, 'B_hf': B_hf, 'gamma': gamma, 'scale': scale}\n","\n","# 4. Adding simple Lorentzians with individual values\n","for i in range(n_lorentz):\n","    prefix = f'l{i+1}_'\n","    initial_vals = get_initial_values('Lorentziana', i+1)\n","\n","    combined_model += Model(lorentzian, prefix=prefix)\n","    params.add_many(\n","        (f'{prefix}amplitude', initial_vals['amplitude'], True, 0, 1),\n","        (f'{prefix}center', initial_vals['center'], True, -2, 2),\n","        (f'{prefix}width', initial_vals['width'], True, 0.05, 0.5)\n","    )\n","\n","# 5. Adding doublets with individual values\n","for i in range(n_doublet):\n","    prefix = f'd{i+1}_'\n","    initial_vals = get_initial_values('Doblete', i+1)\n","\n","    combined_model += Model(doublet_lorentzian, prefix=prefix)\n","    params.add_many(\n","        (f'{prefix}delta', initial_vals['delta'], True, -0.7, 1.5),\n","        (f'{prefix}quad', initial_vals['quad'], True, 0, 2.5),\n","        (f'{prefix}gamma', initial_vals['gamma'], True, 0.0, 0.35),\n","        (f'{prefix}scale', initial_vals['scale'], True, 0.0, 1)\n","    )\n","\n","# 6. Adding sextets with individual values\n","for i in range(n_sextet):\n","    prefix = f'x{i+1}_'\n","    initial_vals = get_initial_values('Sexteto', i+1)\n","\n","    combined_model += Model(sextet_lorentzian, prefix=prefix)\n","    params.add_many(\n","        (f'{prefix}delta', initial_vals['delta'], True, -0.7, 1.5),\n","        (f'{prefix}B_hf', initial_vals['B_hf'], True, 0, 8.5),\n","        (f'{prefix}gamma', initial_vals['gamma'], True, 0.0, 0.35),\n","        (f'{prefix}scale', initial_vals['scale'], True, 0.0, 1)\n","    )\n","\n","# Configure initial parameters and limits\n","\n","def linear_fitting_lmfit(params, x, y):\n","    y_fit= 1-combined_model.eval(params=params, x=x)\n","\n","    return y_fit-y\n","\n","# Extract results and save them in a DataFrame\n","param_headers = []\n","param_values = []\n","param_errors = []\n","\n","# Fitting using the least squares method\n","result = minimize(linear_fitting_lmfit, params, args=(x,y), method='least_squares')\n","best_fit= 1-combined_model.eval(params=result.params, x=x)\n","\n","for name, param in result.params.items():\n","      param_headers.append(name)\n","      param_values.append(param.value)\n","      param_errors.append(param.stderr)\n","\n","# TO CALCULATE AREAS\n","\n","def calculate_areas(x, component_fits):\n","\n","    \"\"\"\n","    Correctly calculates relative areas for Mössbauer components\n","    \"\"\"\n","    areas = {}\n","    total_absorption = 0\n","\n","    # Calculate total absorption area (1 - total transmission)\n","    total_fit = np.zeros_like(x)\n","    for fit in component_fits.values():\n","        total_fit += fit\n","\n","    total_absorption_area = trapz(total_fit, x)\n","\n","    # Calculate the area of ​​each component\n","    for prefix, fit in component_fits.items():\n","        # For Mössbauer, the area is proportional to the integral of (1 - transmission_component)\n","        component_area = trapz(fit, x)\n","        areas[prefix] = (component_area/ total_absorption_area) * 100 if total_absorption_area != 0 else 0\n","\n","    return areas\n","\n","# Use the corrected function\n","component_areas = calculate_areas(x, component_fits)\n","\n","# Calculate error\n","e= (y-best_fit)/y*100\n","\n","# Print adjustment results\n","print(fit_report(result))\n","\n","\"\"\"\n","SPECTRUM AND SUBSPECTRUM GRAPH\n","\"\"\"\n","# DYNAMIC GRAPH\n","plt.style.use('bmh')\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 8), height_ratios=[1,3.5])\n","fig.suptitle(f\"{file} - Mössbauer Plot\")\n","\n","# Error graph\n","ax1.scatter(x, e, c='black', marker='+', s=20)\n","ax1.set_ylim(-1.5, 1.5)\n","ax1.set_ylabel('Error (%)')\n","\n","# Principal graph\n","ax2.scatter(x, y, c='black', marker='+', s=20, label='Data')\n","ax2.plot(x, best_fit, c='red', linewidth=2, label='Total Fit')\n","\n","# Colors for different components\n","colors = ['blue', 'green', 'orange', 'purple', 'brown', 'black', 'gray', 'olive']\n","\n","# Graph each component\n","color_idx = 0\n","for prefix, fit in component_fits.items():\n","    comp_type = prefix[0]  # 'l', 'd' o 'x'\n","    comp_num = prefix[1:]  # '1_', '2_', etc.\n","\n","    if comp_type == 'l':\n","        label = f'Singlet {comp_num}'\n","    elif comp_type == 'd':\n","        label = f'Doublet {comp_num}'\n","    else:\n","        label = f'Sextet {comp_num}'\n","\n","    ax2.plot(x, 1 - fit, c=colors[color_idx % len(colors)], label=label)\n","    color_idx += 1\n","\n","ax2.set_xlabel('Velocity (mm/s)')\n","ax2.set_ylabel('Relative Transmission (a.u.)')\n","ax2.legend(handlelength=2, loc='lower left', shadow=True, fontsize=8)\n","plt.tight_layout()\n","\n","plt.show()\n","\n","\"\"\"\n","GENERATING OUTPUT FILES USING PANDAS\n","\"\"\"\n","\n","# Create a DataFrame with the main data\n","plot_data = pd.DataFrame({\n","    'x': x,\n","    'y': y,\n","    'best_fit': best_fit\n","})\n","\n","# Dynamically add components to the DataFrame\n","# Simple Lorentzians\n","for i in range(n_lorentz):\n","    prefix = f'l{i+1}_'\n","    fit = lorentzian(x, result.params[f'{prefix}amplitude'].value,\n","                    result.params[f'{prefix}center'].value,\n","                    result.params[f'{prefix}width'].value)\n","    plot_data[f'Lorentz_{i+1}'] = fit\n","\n","# Doublets\n","for i in range(n_doublet):\n","    prefix = f'd{i+1}_'\n","    fit = doublet_lorentzian(x, result.params[f'{prefix}delta'].value,\n","                            result.params[f'{prefix}quad'].value,\n","                            result.params[f'{prefix}gamma'].value,\n","                            result.params[f'{prefix}scale'].value)\n","    plot_data[f'Doublet_{i+1}'] = fit\n","\n","# Sextets\n","for i in range(n_sextet):\n","    prefix = f'x{i+1}_'\n","    fit = sextet_lorentzian(x, result.params[f'{prefix}delta'].value,\n","                           result.params[f'{prefix}B_hf'].value,\n","                           result.params[f'{prefix}gamma'].value,\n","                           result.params[f'{prefix}scale'].value)\n","    plot_data[f'Sextet_{i+1}'] = fit\n","\n","# Save DataFrame as CSV\n","plot_data.to_csv(f\"{full}/{file}-plot.csv\", index=False, float_format='%.6e')\n","print(f\"Saved data as: {full}/{file}-plot.csv\")\n","\n","\"\"\"\n","GENERATION OF THE ENHANCED RESULTS DATAFRAME\n","\"\"\"\n","\n","data_for_df = []\n","\n","# Simple Lorentzians\n","for i in range(n_lorentz):\n","    prefix = f'l{i+1}_'\n","    data_for_df.append({\n","        'Phase': f'Lorentz {i+1}',\n","        'Type': 'Lorentzian',\n","        'Amplitude': result.params[f'{prefix}amplitude'].value,\n","        'Center (mm/s)': result.params[f'{prefix}center'].value,\n","        'Width (mm/s)': result.params[f'{prefix}width'].value,\n","        'IS (mm/s)': np.nan,\n","        'Quad Splitting (mm/s)': np.nan,\n","        'Bhf (T)': np.nan,\n","        'Area (%)': component_areas.get(prefix, 0)\n","    })\n","\n","# Doublets\n","for i in range(n_doublet):\n","    prefix = f'd{i+1}_'\n","    data_for_df.append({\n","        'Phase': f'Doublet {i+1}',\n","        'Type': 'Doublet',\n","        'Amplitud': np.nan,\n","        'Center (mm/s)': np.nan,\n","        'Width (mm/s)': result.params[f'{prefix}gamma'].value,\n","        'IS (mm/s)': result.params[f'{prefix}delta'].value,\n","        'Quad Splitting (mm/s)': result.params[f'{prefix}quad'].value,\n","        'Bhf (T)': np.nan,\n","        'Area (%)': component_areas.get(prefix, 0)\n","    })\n","\n","# Sextets\n","for i in range(n_sextet):\n","    prefix = f'x{i+1}_'\n","    Bhf =  33 / 5.312 * result.params[f'{prefix}B_hf'].value\n","    data_for_df.append({\n","        'Phase': f'Sextet {i+1}',\n","        'Type': 'Sextet',\n","        'Amplitude': np.nan,\n","        'Center (mm/s)': np.nan,\n","        'Width (mm/s)': result.params[f'{prefix}gamma'].value,\n","        'IS (mm/s)': result.params[f'{prefix}delta'].value,\n","        'Quad Splitting (mm/s)': np.nan,\n","        'Bhf (T)': Bhf,\n","        'Area (%)': component_areas.get(prefix, 0)\n","    })\n","\n","# Create and save DataFrame\n","df = pd.DataFrame(data_for_df)\n","\n","# Round values ​​for better presentation\n","numeric_columns = ['Amplitude', 'Center (mm/s)', 'Width (mm/s)', 'IS (mm/s)',\n","                   'Quad Splitting (mm/s)', 'Bhf (T)', 'Area (%)']\n","for col in numeric_columns:\n","    if col in df.columns:\n","        df[col] = df[col].round(4)\n","\n","df.to_csv(f\"{full}/{file}-report.csv\", index=False)\n","print(f\"Reporte de parámetros guardado en: {full}/{file}-report.csv\")\n","\n","# Show summary in console\n","print(\"\\n\" + \"=\"*50)\n","print(\"SUMMARY OF THE ADJUSTMENT\")\n","print(\"=\"*50)\n","print(df.to_string(index=False))"]},{"cell_type":"markdown","source":["The following section of the code allows you to identify the phases present in the sample from the fitting results by comparing the subspectral parameters with a CSV database. There is a database of approximately 50 Fe compounds with their Mossbauer spectra for $^{57}Fe$. (This can be skipped if the phases present are known.)"],"metadata":{"id":"cN9O8kKUQPjo"}},{"cell_type":"code","source":["from sklearn.neighbors import NearestNeighbors\n","\n","# 1. Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 2. Load reference data (database)\n","reference_path = '/content/drive/MyDrive/MyFolder/reference_data.csv'  # Assign the right folder where the \"reference_data.csv\" file is located\n","df_ref = pd.read_csv(reference_path)\n","\n","# Function to convert ranges to average values ​​(e.g. \"0.37-0.45\" → 0.41)\n","def parse_value(value):\n","    if isinstance(value, str) and '-' in value:\n","        min_val, max_val = map(float, value.split('-'))\n","        return (min_val + max_val) / 2\n","    return float(value)\n","\n","# Process relevant columns\n","cols = ['IS (mm/s)', 'Quad Splitting (mm/s)', 'Bhf (T)']\n","for col in cols:\n","    df_ref[col] = df_ref[col].apply(parse_value)\n","\n","# 3. Cargar datos experimentales\n","experimental_path = f\"{full}/{file}-report.csv\"  # ¡Ajusta la ruta!\n","df_exp = pd.read_csv(experimental_path)\n","\n","# 4. Preprocess experimental data (handle NaN)\n","X_exp = df_exp[cols].fillna(0).values  # Si Bhf no existe, reemplazar NaN por 0\n","\n","# 5. Training KNN model\n","X_ref = df_ref[cols].values\n","model = NearestNeighbors(n_neighbors=3, metric='euclidean')\n","model.fit(X_ref)\n","\n","# 6. Find matches\n","distances, indices = model.kneighbors(X_exp)\n","\n","# 7. Show results\n","for i, (dist, idx) in enumerate(zip(distances, indices)):\n","    print(f\"\\n Experimental Phase {i+1}:\")\n","    for j, (d, pos) in enumerate(zip(dist, idx)):\n","        compound = df_ref.iloc[pos]['Compound Name']\n","        formula = df_ref.iloc[pos]['Chemical Formula']\n","        is_ref = df_ref.iloc[pos]['IS (mm/s)']\n","        qs_ref = df_ref.iloc[pos]['Quad Splitting (mm/s)']\n","        bhf_ref = df_ref.iloc[pos]['Bhf (T)']\n","        print(f\"  Match {j+1}: {compound} ({formula})\")\n","        print(f\"    IS: {is_ref:.2f} mm/s | QS: {qs_ref:.2f} mm/s | Bhf: {bhf_ref:.1f} T\")\n","        print(f\"    Euclidean Distance: {d:.2f}\\n\")\n","\n","print(\"## Use this result as a guide. It is recommended to have information about the sample's composition and structure. ##\")"],"metadata":{"id":"tJIvF09MobnY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1vVH3cgx109Twd9fTc3s-7AvVDr6gO9Et","timestamp":1740183265912},{"file_id":"1Gbfdr4vHZD5v9r6F3Pc9Rkl7ccQbtLdK","timestamp":1739751107147},{"file_id":"1xENyriLWu-1AlJmG8C4p10s5dZOAc-q6","timestamp":1717204500626},{"file_id":"1CPF0-ooePD0t6eFd64Mu3JbLJqkDsX3r","timestamp":1714790332890}],"mount_file_id":"1CPF0-ooePD0t6eFd64Mu3JbLJqkDsX3r","authorship_tag":"ABX9TyMcfSxbm0+mW2vW7RCJJQsU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}